##Link to DOHMH Github Data
urlfile="https://raw.githubusercontent.com/nychealth/coronavirus-data/master/tests-by-zcta.csv"
##Load data
p <- shapefile("data/raw/nyu_2451_34509/nyu_2451_34509")
d <- read_csv(url(urlfile))
c <- read_csv("data/raw/ACS2015_zctaallvars_modified.csv")
##Remove unncessary columns
p@data <- p@data %>% select(1)
##Remove unncessary columns
d <- d %>% select(1,2,3)
##Rename column header to merge
d <- d %>% rename(zcta = MODZCTA)
##Sum duplicate zipcodes (sometimes happens)
d <- aggregate(list(d$'Positive', d$'Total'), by = list(d$'zcta'), sum)
d <- rename(d, 'zcta' = 1)
d <- rename(d, 'Positive' = 2)
d <- rename(d, 'Total' = 3)
##Merge Zip Code shapefile with Testing data (DOHMH)
m <- merge(p, d, by='zcta')
##Merge Spatial data with census data
m <- merge(m, c, by='zcta')
m$Positive[ is.na(m$Positive)] = 0
m$Total[ is.na(m$Total)] = 0
##Positive per capita
m$positiveperthou <- (m$Positive / m$population)*1000
m$positiveperthou[ is.na(m$positiveperthou)] = 0
m$positiveperthou <- format(round(m$positiveperthou, 2), nsmall = 2)
m$positiveperthou <- as.numeric(m$positiveperthou)
##Total per capita
m$totalperthou <- (m$Total / m$population)*1000
m$totalperthou[is.na(m$totalperthou)] = 0
m$totalperthou <- format(round(m$totalperthou, 2), nsmall = 2)
m$totalperthou <- as.numeric(m$totalperthou)
##Assign CRS
m <- spTransform(m, CRS("+proj=longlat +datum=WGS84 +init=epsg:4269"))
##Write as GeoJSON
writeOGR(m, "data/covid_nyc.js", layer="merged", driver="GeoJSON", overwrite_layer=TRUE)
writeOGR(m, "data/covid_nyc_ogr.js", layer="merged", driver="GeoJSON", overwrite_layer=TRUE)
##Get quantiles for breaks / legend
quantile(m$Positive, probs = seq(0, 1, .20))
##Link to DOHMH Github Data
urlfile="https://raw.githubusercontent.com/nychealth/coronavirus-data/master/case-hosp-death.csv"
##Load data
d <- read_csv(url(urlfile))
##Change NA to 0
d[is.na(d)] <- 0
##Create new columns with cumulative sums
d[,"NEW_COVID_CASE_COUNT_CUM"] <- cumsum(d$NEW_COVID_CASE_COUNT)
d[,"HOSPITALIZED_CASE_COUNT_CUM"] <- cumsum(d$HOSPITALIZED_CASE_COUNT)
d[,"DEATH_COUNT_CUM"] <- cumsum(d$DEATH_COUNT)
##Write as json
write_json(d, "data/case-hosp-death_cumulative.json", pretty = TRUE)
##Write as csv for public use
write.csv(d, "data/case-hosp-death_cumulative.csv", row.names = FALSE)
##Write as GeoJSON
writeOGR(m, "data/covid_nyc_ogr.json", layer="merged", driver="GeoJSON", overwrite_layer=TRUE)
##Load package
library(raster)
library(rgdal)
library(tidyverse)
library(dplyr)
library(readr)
library(jsonlite)
##Link to DOHMH Github Data
urlfile="https://raw.githubusercontent.com/nychealth/coronavirus-data/master/tests-by-zcta.csv"
##Load data
p <- shapefile("data/raw/nyu_2451_34509/nyu_2451_34509")
d <- read_csv(url(urlfile))
c <- read_csv("data/raw/ACS2015_zctaallvars_modified.csv")
##Remove unncessary columns
p@data <- p@data %>% select(1)
##Load package
library(raster)
library(rgdal)
library(tidyverse)
library(dplyr)
library(readr)
library(jsonlite)
##Set working directory
setwd("C:/Users/jaramana/Desktop/covid-tracker-nyc")
##Link to DOHMH Github Data
urlfile="https://raw.githubusercontent.com/nychealth/coronavirus-data/master/tests-by-zcta.csv"
##Load data
p <- shapefile("data/raw/nyu_2451_34509/nyu_2451_34509")
d <- read_csv(url(urlfile))
c <- read_csv("data/raw/ACS2015_zctaallvars_modified.csv")
##Remove unncessary columns
p@data <- p@data %>% select(1)
##Remove unncessary columns
d <- d %>% select(1,2,3)
##Rename column header to merge
d <- d %>% rename(zcta = MODZCTA)
##Sum duplicate zipcodes (sometimes happens)
d <- aggregate(list(d$'Positive', d$'Total'), by = list(d$'zcta'), sum)
d <- rename(d, 'zcta' = 1)
d <- rename(d, 'Positive' = 2)
d <- rename(d, 'Total' = 3)
##Merge Zip Code shapefile with Testing data (DOHMH)
m <- merge(p, d, by='zcta')
##Merge Spatial data with census data
m <- merge(m, c, by='zcta')
m$Positive[ is.na(m$Positive)] = 0
m$Total[ is.na(m$Total)] = 0
##Positive per capita
m$positiveperthou <- (m$Positive / m$population)*1000
m$positiveperthou[ is.na(m$positiveperthou)] = 0
m$positiveperthou <- format(round(m$positiveperthou, 2), nsmall = 2)
m$positiveperthou <- as.numeric(m$positiveperthou)
##Total per capita
m$totalperthou <- (m$Total / m$population)*1000
m$totalperthou[is.na(m$totalperthou)] = 0
m$totalperthou <- format(round(m$totalperthou, 2), nsmall = 2)
m$totalperthou <- as.numeric(m$totalperthou)
##Assign CRS
m <- spTransform(m, CRS("+proj=longlat +datum=WGS84 +init=epsg:4269"))
##Write as GeoJSON
writeOGR(m, "data/covid_nyc_ogr.json", layer="merged", driver="GeoJSON", overwrite_layer=TRUE)
##Get quantiles for breaks / legend
quantile(m$Positive, probs = seq(0, 1, .20))
##Chart--
##Link to DOHMH Github Data
urlfile="https://raw.githubusercontent.com/nychealth/coronavirus-data/master/case-hosp-death.csv"
##Load data
d <- read_csv(url(urlfile))
##Change NA to 0
d[is.na(d)] <- 0
##Create new columns with cumulative sums
d[,"NEW_COVID_CASE_COUNT_CUM"] <- cumsum(d$NEW_COVID_CASE_COUNT)
d[,"HOSPITALIZED_CASE_COUNT_CUM"] <- cumsum(d$HOSPITALIZED_CASE_COUNT)
d[,"DEATH_COUNT_CUM"] <- cumsum(d$DEATH_COUNT)
##Write as json
write_json(d, "data/case-hosp-death_cumulative.json", pretty = TRUE)
##Write as csv for public use
write.csv(d, "data/case-hosp-death_cumulative.csv", row.names = FALSE)
quantile(m$Total, probs = seq(0, 1, .20))
quantile(m$positiveperthou, probs = seq(0, 1, .20))
quantile(m$Total, probs = seq(0, 1, .20))
quantile(m$totalperthou, probs = seq(0, 1, .20))
##Load package
library(raster)
library(rgdal)
library(tidyverse)
library(dplyr)
library(readr)
library(jsonlite)
##Set working directory
setwd("C:/Users/jaramana/Desktop/covid-tracker-nyc")
##Spatial Visualization--
##Link to DOHMH Github Data
urlfile="https://raw.githubusercontent.com/nychealth/coronavirus-data/master/tests-by-zcta.csv"
##Load data
p <- shapefile("data/raw/nyu_2451_34509_filtered/nyu_2451_34509_filtered")
d <- read_csv(url(urlfile))
c <- read_csv("data/raw/ACS2015_zctaallvars_modified.csv")
##Remove unncessary columns
p@data <- p@data %>% select(1)
##Remove unncessary columns
d <- d %>% select(1,2,3)
##Rename column header to merge
d <- d %>% rename(zcta = MODZCTA)
##Sum duplicate zipcodes (sometimes happens)
d <- aggregate(list(d$'Positive', d$'Total'), by = list(d$'zcta'), sum)
d <- rename(d, 'zcta' = 1)
d <- rename(d, 'Positive' = 2)
d <- rename(d, 'Total' = 3)
##Merge Zip Code shapefile with Testing data (DOHMH)
m <- merge(p, d, by='zcta')
##Merge Spatial data with census data
m <- merge(m, c, by='zcta')
m$Positive[ is.na(m$Positive)] = 0
m$Total[ is.na(m$Total)] = 0
##Positive per capita
m$positiveperthou <- (m$Positive / m$population)*1000
m$positiveperthou[ is.na(m$positiveperthou)] = 0
m$positiveperthou <- format(round(m$positiveperthou, 2), nsmall = 2)
m$positiveperthou <- as.numeric(m$positiveperthou)
##Total per capita
m$totalperthou <- (m$Total / m$population)*1000
m$totalperthou[is.na(m$totalperthou)] = 0
m$totalperthou <- format(round(m$totalperthou, 2), nsmall = 2)
m$totalperthou <- as.numeric(m$totalperthou)
##Assign CRS
m <- spTransform(m, CRS("+proj=longlat +datum=WGS84 +init=epsg:4269"))
##Write as GeoJSON
writeOGR(m, "data/covid_nyc_ogr.json", layer="merged", driver="GeoJSON", overwrite_layer=TRUE)
##Get quantiles for breaks / legend
quantile(m$Positive, probs = seq(0, 1, .20))
##Load package
library(raster)
library(rgdal)
library(tidyverse)
library(dplyr)
library(readr)
library(jsonlite)
##Set working directory
setwd("C:/Users/jaramana/Desktop/covid-tracker-nyc")
##Spatial Visualization--
##Link to DOHMH Github Data
urlfile="https://raw.githubusercontent.com/nychealth/coronavirus-data/master/tests-by-zcta.csv"
##Load data
p <- shapefile("data/raw/nyu_2451_34509_filtered/nyu_2451_34509_filtered")
c <- read_csv("data/raw/ACS2015_zctaallvars_modified.csv")
n <- read_csv("data/raw/zcta_neighborhood_names.csv")
d <- read_csv(url(urlfile))
n <- read_csv("data/raw/zcta_neighborhood_names.csv")
n <- read_csv("data/raw/zcta_neighborhood_names.csv")
View(n)
##Remove unncessary columns
p@data <- p@data %>% select(1)
##Remove unncessary columns
d <- d %>% select(1,2,3)
##Rename column header to merge
d <- d %>% rename(zcta = MODZCTA)
##Sum duplicate zipcodes (sometimes happens)
d <- aggregate(list(d$'Positive', d$'Total'), by = list(d$'zcta'), sum)
d <- rename(d, 'zcta' = 1)
d <- rename(d, 'Positive' = 2)
d <- rename(d, 'Total' = 3)
##Merge Zip Code shapefile with Testing data (DOHMH)
m <- merge(p, d, by='zcta')
##Merge Spatial data with census data
m <- merge(m, c, by='zcta')
##Merge Spatial data with census data
m <- merge(m, c, by='zcta')
##Merge Spatial data with neighborhood names
m <- merge(m, n, by='zcta')
View(m)
##Set working directory
setwd("C:/Users/jaramana/Desktop/covid-tracker-nyc")
##Link to DOHMH Github Data
urlfile="https://raw.githubusercontent.com/nychealth/coronavirus-data/master/tests-by-zcta.csv"
##Load data
p <- shapefile("data/raw/nyu_2451_34509/nyu_2451_34509")
d <- read_csv(url(urlfile))
c <- read_csv("data/raw/ACS2015_zctaallvars_modified.csv")
##Remove unncessary columns
p@data <- p@data %>% select(1)
##Remove unncessary columns
d <- d %>% select(1,2,3)
##Rename column header to merge
d <- d %>% rename(zcta = MODZCTA)
##Sum duplicate zipcodes (sometimes happens)
d <- aggregate(list(d$'Positive', d$'Total'), by = list(d$'zcta'), sum)
d <- rename(d, 'zcta' = 1)
d <- rename(d, 'Positive' = 2)
d <- rename(d, 'Total' = 3)
View(c)
##Set working directory
setwd("C:/Users/jaramana/Desktop/covid-tracker-nyc")
##Spatial Visualization--
##Link to DOHMH Github Data
urlfile="https://raw.githubusercontent.com/nychealth/coronavirus-data/master/tests-by-zcta.csv"
##Load data
p <- shapefile("data/raw/nyu_2451_34509_filtered/nyu_2451_34509_filtered")
c <- read_csv("data/raw/ACS2015_zctaallvars_modified.csv")
n <- read_csv("data/raw/zcta_neighborhood_names.csv")
d <- read_csv(url(urlfile))
##Remove unncessary columns
p@data <- p@data %>% select(1)
##Remove unncessary columns
d <- d %>% select(1,2,3)
##Rename column header to merge
d <- d %>% rename(zcta = MODZCTA)
##Sum duplicate zipcodes (sometimes happens)
d <- aggregate(list(d$'Positive', d$'Total'), by = list(d$'zcta'), sum)
d <- rename(d, 'zcta' = 1)
d <- rename(d, 'Positive' = 2)
d <- rename(d, 'Total' = 3)
View(d)
View(n)
##Merge Zip Code shapefile with Testing data (DOHMH)
m <- merge(p, d, by='zcta')
##Merge Spatial data with census data
m <- merge(m, c, by='zcta')
##Merge Spatial data with neighborhood names
m <- merge(m, n , by='zcta')
View(m)
m$Positive[ is.na(m$Positive)] = 0
m$Total[ is.na(m$Total)] = 0
##Positive per capita
m$positiveperthou <- (m$Positive / m$population)*1000
m$positiveperthou[ is.na(m$positiveperthou)] = 0
m$positiveperthou <- format(round(m$positiveperthou, 2), nsmall = 2)
m$positiveperthou <- as.numeric(m$positiveperthou)
##Total per capita
m$totalperthou <- (m$Total / m$population)*1000
m$totalperthou[is.na(m$totalperthou)] = 0
m$totalperthou <- format(round(m$totalperthou, 2), nsmall = 2)
m$totalperthou <- as.numeric(m$totalperthou)
##Assign CRS
m <- spTransform(m, CRS("+proj=longlat +datum=WGS84 +init=epsg:4269"))
##Write as GeoJSON
writeOGR(m, "data/covid_nyc_ogr.json", layer="merged", driver="GeoJSON", overwrite_layer=TRUE)
##Get quantiles for breaks / legend
quantile(m$Positive, probs = seq(0, 1, .20))
View(m)
##Load package
library(raster)
library(rgdal)
library(tidyverse)
library(dplyr)
library(readr)
library(jsonlite)
##Set working directory
setwd("C:/Users/jaramana/Desktop/covid-tracker-nyc")
##Spatial Visualization--
##Link to DOHMH Github Data
urlfile="https://raw.githubusercontent.com/nychealth/coronavirus-data/master/tests-by-zcta.csv"
##Load data
p <- shapefile("data/raw/nyu_2451_34509_filtered/nyu_2451_34509_filtered")
d <- read_csv(url(urlfile))
c <- read_csv("data/raw/ACS2015_zctaallvars_modified.csv")
##Remove unncessary columns
p@data <- p@data %>% select(1)
##Remove unncessary columns
d <- d %>% select(1,2,3)
##Rename column header to merge
d <- d %>% rename(zcta = MODZCTA)
##Sum duplicate zipcodes (sometimes happens)
d <- aggregate(list(d$'Positive', d$'Total'), by = list(d$'zcta'), sum)
d <- rename(d, 'zcta' = 1)
d <- rename(d, 'Positive' = 2)
d <- rename(d, 'Total' = 3)
##Merge Zip Code shapefile with Testing data (DOHMH)
m <- merge(p, d, by='zcta')
##Merge Spatial data with census data
m <- merge(m, c, by='zcta')
m$Positive[ is.na(m$Positive)] = 0
m$Total[ is.na(m$Total)] = 0
##Positive per capita
m$positiveperthou <- (m$Positive / m$population)*1000
m$positiveperthou[ is.na(m$positiveperthou)] = 0
m$positiveperthou <- format(round(m$positiveperthou, 2), nsmall = 2)
m$positiveperthou <- as.numeric(m$positiveperthou)
##Total per capita
m$totalperthou <- (m$Total / m$population)*1000
m$totalperthou[is.na(m$totalperthou)] = 0
m$totalperthou <- format(round(m$totalperthou, 2), nsmall = 2)
m$totalperthou <- as.numeric(m$totalperthou)
##Assign CRS
m <- spTransform(m, CRS("+proj=longlat +datum=WGS84 +init=epsg:4269"))
##Write as GeoJSON
writeOGR(m, "data/covid_nyc_ogr.json", layer="merged", driver="GeoJSON", overwrite_layer=TRUE)
##Get quantiles for breaks / legend
quantile(m$Positive, probs = seq(0, 1, .20))
##Chart--
##Link to DOHMH Github Data
urlfile="https://raw.githubusercontent.com/nychealth/coronavirus-data/master/case-hosp-death.csv"
##Load data
d <- read_csv(url(urlfile))
##Change NA to 0
d[is.na(d)] <- 0
##Create new columns with cumulative sums
d[,"NEW_COVID_CASE_COUNT_CUM"] <- cumsum(d$NEW_COVID_CASE_COUNT)
d[,"HOSPITALIZED_CASE_COUNT_CUM"] <- cumsum(d$HOSPITALIZED_CASE_COUNT)
d[,"DEATH_COUNT_CUM"] <- cumsum(d$DEATH_COUNT)
##Write as json
write_json(d, "data/case-hosp-death_cumulative.json", pretty = TRUE)
##Write as csv for public use
write.csv(d, "data/case-hosp-death_cumulative.csv", row.names = FALSE)
##Load package
library(raster)
library(rgdal)
library(tidyverse)
library(dplyr)
library(readr)
library(jsonlite)
##Set working directory
setwd("C:/Users/jaramana/Desktop/covid-tracker-nyc")
##Link to DOHMH Github Data
urlfile="https://raw.githubusercontent.com/nychealth/coronavirus-data/master/tests-by-zcta.csv"
##Load data
p <- shapefile("data/raw/nyu_2451_34509_filtered/nyu_2451_34509_filtered")
d <- read_csv(url(urlfile))
c <- read_csv("data/raw/ACS2015_zctaallvars_modified.csv")
n <- read_csv("data/raw/zcta_neighborhood_names.csv")
##Remove unncessary columns
p@data <- p@data %>% select(1)
##Remove unncessary columns
d <- d %>% select(1,2,3)
##Rename column header to merge
d <- d %>% rename(zcta = MODZCTA)
##Sum duplicate zipcodes (sometimes happens)
d <- aggregate(list(d$'Positive', d$'Total'), by = list(d$'zcta'), sum)
d <- rename(d, 'zcta' = 1)
d <- rename(d, 'Positive' = 2)
d <- rename(d, 'Total' = 3)
##Merge Zip Code shapefile with Testing data (DOHMH)
m <- merge(p, d, by='zcta')
##Merge Spatial data with neighborhood names
m <- merge(m, n, by='zcta')
##Merge Spatial data with census data
m <- merge(m, c, by='zcta')
m$Positive[ is.na(m$Positive)] = 0
m$Total[ is.na(m$Total)] = 0
##Positive per capita
m$positiveperthou <- (m$Positive / m$population)*1000
m$positiveperthou[ is.na(m$positiveperthou)] = 0
m$positiveperthou <- format(round(m$positiveperthou, 2), nsmall = 2)
m$positiveperthou <- as.numeric(m$positiveperthou)
##Total per capita
m$totalperthou <- (m$Total / m$population)*1000
m$totalperthou[is.na(m$totalperthou)] = 0
m$totalperthou <- format(round(m$totalperthou, 2), nsmall = 2)
m$totalperthou <- as.numeric(m$totalperthou)
##Assign CRS
m <- spTransform(m, CRS("+proj=longlat +datum=WGS84 +init=epsg:4269"))
##Write as GeoJSON
writeOGR(m, "data/covid_nyc_ogr.json", layer="merged", driver="GeoJSON", overwrite_layer=TRUE)
##Get quantiles for breaks / legend
quantile(m$Positive, probs = seq(0, 1, .20))
##Load package
library(raster)
library(rgdal)
library(tidyverse)
library(dplyr)
library(readr)
library(jsonlite)
##Set working directory
setwd("C:/Users/jaramana/Desktop/covid-tracker-nyc")
##Link to DOHMH Github Data
urlfile="https://raw.githubusercontent.com/nychealth/coronavirus-data/master/tests-by-zcta.csv"
##Load data
p <- shapefile("data/raw/nyu_2451_34509_filtered/nyu_2451_34509_filtered")
d <- read_csv(url(urlfile))
c <- read_csv("data/raw/ACS2015_zctaallvars_modified.csv")
n <- read_csv("data/raw/zcta_neighborhood_names.csv")
##Remove unncessary columns
p@data <- p@data %>% select(1)
##Remove unncessary columns
d <- d %>% select(1,2,3)
##Rename column header to merge
d <- d %>% rename(zcta = MODZCTA)
##Sum duplicate zipcodes (sometimes happens)
d <- aggregate(list(d$'Positive', d$'Total'), by = list(d$'zcta'), sum)
d <- rename(d, 'zcta' = 1)
d <- rename(d, 'Positive' = 2)
d <- rename(d, 'Total' = 3)
##Merge Zip Code shapefile with Testing data (DOHMH)
m <- merge(p, d, by='zcta')
##Merge Spatial data with neighborhood names
m <- merge(m, n, by='zcta')
##Merge Spatial data with census data
m <- merge(m, c, by='zcta')
m$Positive[ is.na(m$Positive)] = 0
m$Total[ is.na(m$Total)] = 0
##Positive per capita
m$positiveperthou <- (m$Positive / m$population)*1000
m$positiveperthou[ is.na(m$positiveperthou)] = 0
m$positiveperthou <- format(round(m$positiveperthou, 2), nsmall = 2)
m$positiveperthou <- as.numeric(m$positiveperthou)
##Total per capita
m$totalperthou <- (m$Total / m$population)*1000
m$totalperthou[is.na(m$totalperthou)] = 0
m$totalperthou <- format(round(m$totalperthou, 2), nsmall = 2)
m$totalperthou <- as.numeric(m$totalperthou)
##Assign CRS
m <- spTransform(m, CRS("+proj=longlat +datum=WGS84 +init=epsg:4269"))
##Write as GeoJSON
writeOGR(m, "data/covid_nyc_ogr.json", layer="merged", driver="GeoJSON", overwrite_layer=TRUE)
##Get quantiles for breaks / legend
quantile(m$Positive, probs = seq(0, 1, .20))
##Link to DOHMH Github Data
urlfile="https://raw.githubusercontent.com/nychealth/coronavirus-data/master/case-hosp-death.csv"
##Load data
d <- read_csv(url(urlfile))
##Change NA to 0
d[is.na(d)] <- 0
##Create new columns with cumulative sums
d[,"NEW_COVID_CASE_COUNT_CUM"] <- cumsum(d$NEW_COVID_CASE_COUNT)
d[,"HOSPITALIZED_CASE_COUNT_CUM"] <- cumsum(d$HOSPITALIZED_CASE_COUNT)
d[,"DEATH_COUNT_CUM"] <- cumsum(d$DEATH_COUNT)
##Write as json
write_json(d, "data/case-hosp-death_cumulative.json", pretty = TRUE)
##Write as csv for public use
write.csv(d, "data/case-hosp-death_cumulative.csv", row.names = FALSE)
##Link to DOHMH Github Data
urlfile="https://raw.githubusercontent.com/nychealth/coronavirus-data/master/case-hosp-death.csv"
##Load data
d <- read_csv(url(urlfile))
##Change NA to 0
d[is.na(d)] <- 0
##Create new columns with cumulative sums
d[,"NEW_COVID_CASE_COUNT_CUM"] <- cumsum(d$NEW_COVID_CASE_COUNT)
d[,"HOSPITALIZED_CASE_COUNT_CUM"] <- cumsum(d$HOSPITALIZED_CASE_COUNT)
d[,"DEATH_COUNT_CUM"] <- cumsum(d$DEATH_COUNT)
View(d)
##Link to DOHMH Github Data
urlfile="https://raw.githubusercontent.com/nychealth/coronavirus-data/master/case-hosp-death.csv"
##Load data
d <- read_csv(url(urlfile))
View(d)
##Link to DOHMH Github Data
urlfile="https://raw.githubusercontent.com/nychealth/coronavirus-data/master/case-hosp-death.csv"
##Load data
d <- read_csv(url(urlfile))
##Change NA to 0
d[is.na(d)] <- 0
##Rename column header (sometimes DOHMH messes this up)
d <- d %>% rename(zcta = MODZCTA)
d <- rename(d, 'DATE_OF_INTEREST' = 1)
View(d)
##Rename column header (sometimes DOHMH messes this up)
d <- rename(d, 'DATE_OF_INTEREST' = 1)
##Create new columns with cumulative sums
d[,"NEW_COVID_CASE_COUNT_CUM"] <- cumsum(d$NEW_COVID_CASE_COUNT)
d[,"HOSPITALIZED_CASE_COUNT_CUM"] <- cumsum(d$HOSPITALIZED_CASE_COUNT)
d[,"DEATH_COUNT_CUM"] <- cumsum(d$DEATH_COUNT)
##Write as json
write_json(d, "data/case-hosp-death_cumulative.json", pretty = TRUE)
##Write as csv for public use
write.csv(d, "data/case-hosp-death_cumulative.csv", row.names = FALSE)
