##Set working directory
setwd("C:/Users/jaramana/Desktop/COVID Tracker NYC")
##Link to DOHMH Github Data
urlfile="https://raw.githubusercontent.com/nychealth/coronavirus-data/master/tests-by-zcta.csv"
##Load data
p <- shapefile("data/raw/nyu_2451_34509/nyu_2451_34509")
d <- read_csv(url(urlfile))
c <- read_csv("data/raw/ACS2015_zctaallvars_modified")
d <- read_csv(url(urlfile))
c <- read_csv("data/raw/ACS2015_zctaallvars_modified.csv")
##Remove unncessary columns
p@data <- p@data %>% select(1)
##Remove unncessary columns
d <- d %>% select(1,2,3)
##Rename column header to merge
d <- d %>% rename(zcta = MODZCTA)
##Merge Zip Code shapefile with Testing data (DOHMH)
m <- merge(p, d, by='zcta')
##Merge Spatial data with census data
m <- merge(m, c, by='zcta')
m$Positive[ is.na(m$Positive)] = 0
m$Total[ is.na(m$Total)] = 0
##Positive per capita
m$positiveperthou <- (m$Positive / m$population)*1000
m$positiveperthou[ is.na(m$positiveperthou)] = 0
m$positiveperthou <- format(round(m$positiveperthou, 2), nsmall = 2)
m$positiveperthou <- as.numeric(m$positiveperthou)
##Total per capita
m$totalperthou <- (m$Total / m$population)*1000
m$totalperthou[is.na(m$totalperthou)] = 0
m$totalperthou <- format(round(m$totalperthou, 2), nsmall = 2)
m$totalperthou <- as.numeric(m$totalperthou)
##Assign CRS
m <- spTransform(m, CRS("+proj=longlat +datum=WGS84 +init=epsg:4269"))
##Write as GeoJSON
writeOGR(m, "data/covid_nyc_test.js", layer="merged", driver="GeoJSON", overwrite_layer=TRUE)
##addendum
##Get quantiles for breaks / legend
quantile(m$totalperthou, probs = seq(0, 1, .20))
quantile(m$positiveperthou, probs = seq(0, 1, .20))
##addendum
##Get quantiles for breaks / legend
quantile(m$positiveperthou, probs = seq(0, 1, .25))
quantile(m$totalperthou, probs = seq(0, 1, .25))
quantile(m$Total, probs = seq(0, 1, .25))
##addendum
##Get quantiles for breaks / legend
quantile(m$Total, probs = seq(0, 1, .20))
quantile(m$totalperthou, probs = seq(0, 1, .20))
quantile(m$positiveperthou, probs = seq(0, 1, .20))
quantile(m$Positive, probs = seq(0, 1, .20))
##Load package
library(raster)
library(rgdal)
library(tidyverse)
library(dplyr)
library (readr)
##Set working directory
setwd("C:/Users/jaramana/Desktop/COVID Tracker NYC")
##Link to DOHMH Github Data
urlfile="https://raw.githubusercontent.com/nychealth/coronavirus-data/master/tests-by-zcta.csv"
##Load data
p <- shapefile("data/raw/nyu_2451_34509/nyu_2451_34509")
d <- read_csv(url(urlfile))
c <- read_csv("data/raw/ACS2015_zctaallvars_modified.csv")
##Remove unncessary columns
p@data <- p@data %>% select(1)
##Remove unncessary columns
d <- d %>% select(1,2,3)
##Rename column header to merge
d <- d %>% rename(zcta = MODZCTA)
##Merge Zip Code shapefile with Testing data (DOHMH)
m <- merge(p, d, by='zcta')
##Merge Spatial data with census data
m <- merge(m, c, by='zcta')
m$Positive[ is.na(m$Positive)] = 0
m$Total[ is.na(m$Total)] = 0
##Positive per capita
m$positiveperthou <- (m$Positive / m$population)*1000
m$positiveperthou[ is.na(m$positiveperthou)] = 0
m$positiveperthou <- format(round(m$positiveperthou, 2), nsmall = 2)
m$positiveperthou <- as.numeric(m$positiveperthou)
##Total per capita
m$totalperthou <- (m$Total / m$population)*1000
m$totalperthou[is.na(m$totalperthou)] = 0
m$totalperthou <- format(round(m$totalperthou, 2), nsmall = 2)
m$totalperthou <- as.numeric(m$totalperthou)
##Assign CRS
m <- spTransform(m, CRS("+proj=longlat +datum=WGS84 +init=epsg:4269"))
##Write as GeoJSON
writeOGR(m, "data/covid_nyc_test.js", layer="merged", driver="GeoJSON", overwrite_layer=TRUE)
##Load package
library(raster)
library(rgdal)
library(tidyverse)
library(dplyr)
library (readr)
##Set working directory
setwd("C:/Users/jaramana/Desktop/COVID Tracker NYC")
##Link to DOHMH Github Data
urlfile="https://raw.githubusercontent.com/nychealth/coronavirus-data/master/tests-by-zcta.csv"
##Load data
p <- shapefile("data/raw/nyu_2451_34509/nyu_2451_34509")
d <- read_csv(url(urlfile))
c <- read_csv("data/raw/ACS2015_zctaallvars_modified.csv")
##Remove unncessary columns
p@data <- p@data %>% select(1)
##Remove unncessary columns
d <- d %>% select(1,2,3)
##Rename column header to merge
d <- d %>% rename(zcta = MODZCTA)
##Merge Zip Code shapefile with Testing data (DOHMH)
m <- merge(p, d, by='zcta')
##Merge Spatial data with census data
m <- merge(m, c, by='zcta')
m$Positive[ is.na(m$Positive)] = 0
m$Total[ is.na(m$Total)] = 0
##Positive per capita
m$positiveperthou <- (m$Positive / m$population)*1000
m$positiveperthou[ is.na(m$positiveperthou)] = 0
m$positiveperthou <- format(round(m$positiveperthou, 2), nsmall = 2)
m$positiveperthou <- as.numeric(m$positiveperthou)
##Total per capita
m$totalperthou <- (m$Total / m$population)*1000
m$totalperthou[is.na(m$totalperthou)] = 0
m$totalperthou <- format(round(m$totalperthou, 2), nsmall = 2)
m$totalperthou <- as.numeric(m$totalperthou)
##Assign CRS
m <- spTransform(m, CRS("+proj=longlat +datum=WGS84 +init=epsg:4269"))
##Write as GeoJSON
writeOGR(m, "data/covid_nyc.js", layer="merged", driver="GeoJSON", overwrite_layer=TRUE)
##addendum
##Get quantiles for breaks / legend
quantile(m$Positive, probs = seq(0, 1, .20))
writeOGR(m, "data/covid_nyc.js", layer="merged", driver="GeoJSON", overwrite_layer=TRUE)
writeOGR(m, "data/covid_nyc.js", layer="merged", driver="GeoJSON", overwrite_layer=TRUE)
writeOGR(m, "data/covid_nyc.js", layer="merged", driver="GeoJSON", overwrite_layer=TRUE)
writeOGR(m, "data/raw/covid_nyc.js", layer="merged", driver="GeoJSON", overwrite_layer=TRUE)
##Load package
library(raster)
library(rgdal)
library(tidyverse)
library(dplyr)
library (readr)
##Set working directory
setwd("C:/Users/jaramana/Desktop/COVID Tracker NYC")
##Link to DOHMH Github Data
urlfile="https://raw.githubusercontent.com/nychealth/coronavirus-data/master/case-hosp-death.csv"
##Load data
d <- read_csv(url(urlfile))
View(d)
data.frame(HOSPITALIZED_CASE_COUNT, c=cumsum(HOSPITALIZED_CASE_COUNT))
data.frame(d, c=cumsum(HOSPITALIZED_CASE_COUNT))
data.frame(d$HOSPITALIZED_CASE_COUNT, d$c=cumsum(d$HOSPITALIZED_CASE_COUNT))
data.frame(d$HOSPITALIZED_CASE_COUNT, d$hello=cumsum(d$HOSPITALIZED_CASE_COUNT))
data.frame(d$HOSPITALIZED_CASE_COUNT, d$hello==cumsum(d$HOSPITALIZED_CASE_COUNT))
data.frame(d$HOSPITALIZED_CASE_COUNT, d$hello=cumsum(d$HOSPITALIZED_CASE_COUNT))
##Load package
library(raster)
library(rgdal)
library(tidyverse)
library(dplyr)
library (readr)
data.frame(d$HOSPITALIZED_CASE_COUNT, d$hello=cumsum(d$HOSPITALIZED_CASE_COUNT))
d$HOSPITALIZED_CASE_COUNT
d$hello
d <- d$test
##Load data
d <- read_csv(url(urlfile))
d$test <- 0
View(d)
data.frame(d$HOSPITALIZED_CASE_COUNT, d$hello=cumsum(d$HOSPITALIZED_CASE_COUNT))
data.frame(d$HOSPITALIZED_CASE_COUNT, d$test=cumsum(d$HOSPITALIZED_CASE_COUNT))
data.frame(d$HOSPITALIZED_CASE_COUNT, d$DEATH_COUNT=dplyr::lag(d$HOSPITALIZED_CASE_COUNT))
data.frame(d$3, d$5=cumsum(d$3))
cumsum(d$HOSPITALIZED_CASE_COUNT)
product_sales[,"cum_sales"] <- cumsum(d$HOSPITALIZED_CASE_COUNT)
View(d)
d[,"cum_sales"] <- cumsum(d$HOSPITALIZED_CASE_COUNT)
##Load data
d <- read_csv(url(urlfile))
d[,"HOSPITALIZED_CASE_COUNT_CUM"] <- cumsum(d$HOSPITALIZED_CASE_COUNT)
d[,"NEW_COVID_CASE_COUNT_CUM"] <- cumsum(d$NEW_COVID_CASE_COUNT)
d[,"DEATH_COUNT"] <- cumsum(d$DEATH_COUNT)
View(d)
d <- read_csv(url(urlfile))
d[,"HOSPITALIZED_CASE_COUNT_CUM"] <- cumsum(d$HOSPITALIZED_CASE_COUNT)
d[,"NEW_COVID_CASE_COUNT_CUM"] <- cumsum(d$NEW_COVID_CASE_COUNT)
d[,"DEATH_COUNT_CUM"] <- cumsum(d$DEATH_COUNT)
View(d)
d <- read_csv(url(urlfile))
d[is.na(d)] <- 0
d[,"HOSPITALIZED_CASE_COUNT_CUM"] <- cumsum(d$HOSPITALIZED_CASE_COUNT)
d[,"NEW_COVID_CASE_COUNT_CUM"] <- cumsum(d$NEW_COVID_CASE_COUNT)
d[,"DEATH_COUNT_CUM"] <- cumsum(d$DEATH_COUNT)
View(d)
##Load data
d <- read_csv(url(urlfile))
d[is.na(d)] <- 0
d[,"NEW_COVID_CASE_COUNT_CUM"] <- cumsum(d$NEW_COVID_CASE_COUNT)
d[,"HOSPITALIZED_CASE_COUNT_CUM"] <- cumsum(d$HOSPITALIZED_CASE_COUNT)
d[,"DEATH_COUNT_CUM"] <- cumsum(d$DEATH_COUNT)
write_json(d, data/covid_nyc_chart.js, pretty = TRUE, na = FALSE, auto_unbox = FALSE)
library(jsonlite)
write_csv(d, "data/case-hosp-death_cumulative.csv", overwrite = TRUE)
write.csv(d, "data/case-hosp-death_cumulative.csv", overwrite = TRUE)
##Write as csv for public use
write.csv(d, "data/case-hosp-death_cumulative.csv")
##Write as csv for public use
write.csv(d, "data/case-hosp-death_cumulative.csv", row.names = FALSE)
##Write as json
write_json(d, "data/covid_nyc_chart.js", pretty = TRUE, na = FALSE, auto_unbox = FALSE)
##Write as json
write.json(d, "data/covid_nyc_chart.js")
##Write as json
write.json(d, "data/covid_nyc_chart.js", pretty = TRUE, na = TRUE, auto_unbox = FALSE)
write_json(d, "data/covid_nyc_chart.js", pretty = TRUE, na = TRUE, auto_unbox = FALSE)
##Write as json
write_json(d, "data/covid_nyc_chart.jsON"
##Write as json
write_json(d, "data/covid_nyc_chart.json")
##Write as json
write_json(d, "data/covid_nyc_chart.json")
##Write as json
write_json(d, "data/case-hosp-death_cumulative.json")
write_json(d, "data/case-hosp-death_cumulative.json")
write_json(d, "data/case-hosp-death_cumulative.json", pretty = TRUE)
##Load package
library(raster)
library(rgdal)
library(tidyverse)
library(dplyr)
library(readr)
library(jsonlite)
##Set working directory
setwd("C:/Users/jaramana/Desktop/COVID Tracker NYC")
##Link to DOHMH Github Data
urlfile="https://raw.githubusercontent.com/nychealth/coronavirus-data/master/tests-by-zcta.csv"
##Load data
p <- shapefile("data/raw/nyu_2451_34509/nyu_2451_34509")
d <- read_csv(url(urlfile))
c <- read_csv("data/raw/ACS2015_zctaallvars_modified.csv")
##Remove unncessary columns
p@data <- p@data %>% select(1)
##Remove unncessary columns
d <- d %>% select(1,2,3)
##Rename column header to merge
d <- d %>% rename(zcta = MODZCTA)
##Merge Zip Code shapefile with Testing data (DOHMH)
m <- merge(p, d, by='zcta')
##Merge Spatial data with census data
m <- merge(m, c, by='zcta')
m$Positive[ is.na(m$Positive)] = 0
m$Total[ is.na(m$Total)] = 0
##Positive per capita
m$positiveperthou <- (m$Positive / m$population)*1000
m$positiveperthou[ is.na(m$positiveperthou)] = 0
m$positiveperthou <- format(round(m$positiveperthou, 2), nsmall = 2)
m$positiveperthou <- as.numeric(m$positiveperthou)
##Total per capita
m$totalperthou <- (m$Total / m$population)*1000
m$totalperthou[is.na(m$totalperthou)] = 0
m$totalperthou <- format(round(m$totalperthou, 2), nsmall = 2)
m$totalperthou <- as.numeric(m$totalperthou)
##Assign CRS
m <- spTransform(m, CRS("+proj=longlat +datum=WGS84 +init=epsg:4269"))
View(d)
View(p)
##Load package
library(raster)
library(rgdal)
library(tidyverse)
library(dplyr)
library(readr)
library(jsonlite)
##Set working directory
setwd("C:/Users/jaramana/Desktop/covid-tracker-nyc")
##Link to DOHMH Github Data
urlfile="https://raw.githubusercontent.com/nychealth/coronavirus-data/master/tests-by-zcta.csv"
##Load data
p <- shapefile("data/raw/nyu_2451_34509_filtered/nyu_2451_34509_filtered")
d <- read_csv(url(urlfile))
c <- read_csv("data/raw/ACS2015_zctaallvars_modified.csv")
n <- read_csv("data/raw/zcta_neighborhood_names.csv")
View(n)
##Remove unncessary columns
p@data <- p@data %>% select(1)
##Remove unncessary columns
d <- d %>% select(1,2,3)
##Rename column header to merge
d <- d %>% rename(zcta = MODZCTA)
##Sum duplicate zipcodes (sometimes happens)
d <- aggregate(list(d$'Positive', d$'Total'), by = list(d$'zcta'), sum)
d <- rename(d, 'zcta' = 1)
d <- rename(d, 'Positive' = 2)
d <- rename(d, 'Total' = 3)
##Merge Zip Code shapefile with Testing data (DOHMH)
m <- merge(p, d, by='zcta')
##Merge Spatial data with neighborhood names
m <- merge(m, n, by='zcta')
##Merge Spatial data with census data
m <- merge(m, c, by='zcta')
m$Positive[ is.na(m$Positive)] = 0
m$Total[ is.na(m$Total)] = 0
##Positive per capita
m$positiveperthou <- (m$Positive / m$population)*1000
m$positiveperthou[ is.na(m$positiveperthou)] = 0
m$positiveperthou <- format(round(m$positiveperthou, 2), nsmall = 2)
m$positiveperthou <- as.numeric(m$positiveperthou)
##Total per capita
m$totalperthou <- (m$Total / m$population)*1000
m$totalperthou[is.na(m$totalperthou)] = 0
m$totalperthou <- format(round(m$totalperthou, 2), nsmall = 2)
m$totalperthou <- as.numeric(m$totalperthou)
##Assign CRS
m <- spTransform(m, CRS("+proj=longlat +datum=WGS84 +init=epsg:4269"))
##Write as GeoJSON
writeOGR(m, "data/covid_nyc.json", layer="merged", driver="GeoJSON", overwrite_layer=TRUE)
##Get quantiles for breaks / legend
quantile(m$Positive, probs = seq(0, 1, .20))
##Link to DOHMH Github Data
urlfile="https://raw.githubusercontent.com/nychealth/coronavirus-data/master/case-hosp-death.csv"
##Load data
d <- read_csv(url(urlfile))
##Change NA to 0
d[is.na(d)] <- 0
##Rename column header (sometimes DOHMH messes this up)
d <- rename(d, 'DATE_OF_INTEREST' = 1)
##Create new columns with cumulative sums
d[,"NEW_COVID_CASE_COUNT_CUM"] <- cumsum(d$NEW_COVID_CASE_COUNT)
d[,"HOSPITALIZED_CASE_COUNT_CUM"] <- cumsum(d$HOSPITALIZED_CASE_COUNT)
d[,"DEATH_COUNT_CUM"] <- cumsum(d$DEATH_COUNT)
##Write as json
write_json(d, "data/case-hosp-death_cumulative.json", pretty = TRUE)
##Write as csv for public use
write.csv(d, "download/case-hosp-death_cumulative.csv", row.names = FALSE)
##Load package
library(raster)
library(rgdal)
library(tidyverse)
library(dplyr)
library(readr)
library(jsonlite)
##Set working directory
setwd("C:/Users/jaramana/Desktop/covid-tracker-nyc")
##Link to DOHMH Github Data
urlfile="https://raw.githubusercontent.com/nychealth/coronavirus-data/master/tests-by-zcta.csv"
##Load data
p <- shapefile("data/raw/nyu_2451_34509_filtered/nyu_2451_34509_filtered")
d <- read_csv(url(urlfile))
c <- read_csv("data/raw/ACS2015_zctaallvars_modified.csv")
n <- read_csv("data/raw/zcta_neighborhood_names.csv")
##Remove unncessary columns
p@data <- p@data %>% select(1)
##Remove unncessary columns
d <- d %>% select(1,2,3)
##Rename column header to merge
d <- d %>% rename(zcta = MODZCTA)
##Sum duplicate zipcodes (sometimes happens)
d <- aggregate(list(d$'Positive', d$'Total'), by = list(d$'zcta'), sum)
d <- rename(d, 'zcta' = 1)
d <- rename(d, 'Positive' = 2)
d <- rename(d, 'Total' = 3)
##Merge Zip Code shapefile with Testing data (DOHMH)
m <- merge(p, d, by='zcta')
View(n)
View(m)
##Merge Spatial data with neighborhood names
m <- merge(m, n, by='zcta')
View(m)
##Load package
library(raster)
library(rgdal)
library(tidyverse)
library(dplyr)
library(readr)
library(jsonlite)
##Set working directory
setwd("C:/Users/jaramana/Desktop/covid-tracker-nyc")
##Link to DOHMH Github Data
urlfile="https://raw.githubusercontent.com/nychealth/coronavirus-data/master/tests-by-zcta.csv"
##Load data
p <- shapefile("data/raw/nyu_2451_34509_filtered/nyu_2451_34509_filtered")
d <- read_csv(url(urlfile))
c <- read_csv("data/raw/ACS2015_zctaallvars_modified.csv")
n <- read_csv("data/raw/zcta_neighborhood_names.csv")
View(n)
##Remove unncessary columns
p@data <- p@data %>% select(1)
##Remove unncessary columns
d <- d %>% select(1,2,3)
##Rename column header to merge
d <- d %>% rename(zcta = MODZCTA)
##Sum duplicate zipcodes (sometimes happens)
d <- aggregate(list(d$'Positive', d$'Total'), by = list(d$'zcta'), sum)
d <- rename(d, 'zcta' = 1)
d <- rename(d, 'Positive' = 2)
d <- rename(d, 'Total' = 3)
##Merge Zip Code shapefile with Testing data (DOHMH)
m <- merge(p, d, by='zcta')
##Merge Spatial data with neighborhood names
m <- merge(m, n, by='zcta')
##Merge Spatial data with census data
m <- merge(m, c, by='zcta')
m$Positive[ is.na(m$Positive)] = 0
m$Total[ is.na(m$Total)] = 0
##Positive per capita
m$positiveperthou <- (m$Positive / m$population)*1000
m$positiveperthou[ is.na(m$positiveperthou)] = 0
m$positiveperthou <- format(round(m$positiveperthou, 2), nsmall = 2)
m$positiveperthou <- as.numeric(m$positiveperthou)
##Total per capita
m$totalperthou <- (m$Total / m$population)*1000
m$totalperthou[is.na(m$totalperthou)] = 0
m$totalperthou <- format(round(m$totalperthou, 2), nsmall = 2)
m$totalperthou <- as.numeric(m$totalperthou)
##Assign CRS
m <- spTransform(m, CRS("+proj=longlat +datum=WGS84 +init=epsg:4269"))
##Write as GeoJSON
writeOGR(m, "data/covid_nyc.json", layer="merged", driver="GeoJSON", overwrite_layer=TRUE)
##Get quantiles for breaks / legend
quantile(m$Positive, probs = seq(0, 1, .20))
##Link to DOHMH Github Data
urlfile="https://raw.githubusercontent.com/nychealth/coronavirus-data/master/case-hosp-death.csv"
##Load data
d <- read_csv(url(urlfile))
##Change NA to 0
d[is.na(d)] <- 0
##Rename column header (sometimes DOHMH messes this up)
d <- rename(d, 'DATE_OF_INTEREST' = 1)
##Create new columns with cumulative sums
d[,"NEW_COVID_CASE_COUNT_CUM"] <- cumsum(d$NEW_COVID_CASE_COUNT)
d[,"HOSPITALIZED_CASE_COUNT_CUM"] <- cumsum(d$HOSPITALIZED_CASE_COUNT)
d[,"DEATH_COUNT_CUM"] <- cumsum(d$DEATH_COUNT)
##Write as json
write_json(d, "data/case-hosp-death_cumulative.json", pretty = TRUE)
##Write as csv for public use
write.csv(d, "download/case-hosp-death_cumulative.csv", row.names = FALSE)
quantile(m$Positive, probs = seq(0, 1, .20))
quantile(m$positiveperthou, probs = seq(0, 1, .20))
quantile(m$Total, probs = seq(0, 1, .20))
totalperthou(m$Total, probs = seq(0, 1, .20))
quantile(m$totalperthou, probs = seq(0, 1, .20))
##Load package
library(raster)
library(rgdal)
library(tidyverse)
library(dplyr)
library(readr)
library(jsonlite)
##Set working directory
setwd("C:/Users/jaramana/Desktop/covid-tracker-nyc")
##Spatial Visualization--
##Link to DOHMH Github Data
urlfile="https://raw.githubusercontent.com/nychealth/coronavirus-data/master/tests-by-zcta.csv"
##Load data
p <- shapefile("data/raw/nyu_2451_34509_filtered/nyu_2451_34509_filtered")
d <- read_csv(url(urlfile))
c <- read_csv("data/raw/ACS2015_zctaallvars_modified.csv")
n <- read_csv("data/raw/zcta_neighborhood_names.csv")
##Remove unncessary columns
p@data <- p@data %>% select(1)
##Remove unncessary columns
d <- d %>% select(1,2,3)
##Rename column header to merge
d <- d %>% rename(zcta = MODZCTA)
##Sum duplicate zipcodes (sometimes happens)
d <- aggregate(list(d$'Positive', d$'Total'), by = list(d$'zcta'), sum)
d <- rename(d, 'zcta' = 1)
d <- rename(d, 'Positive' = 2)
d <- rename(d, 'Total' = 3)
##Merge Zip Code shapefile with Testing data (DOHMH)
m <- merge(p, d, by='zcta')
##Merge Spatial data with neighborhood names
m <- merge(m, n, by='zcta')
##Merge Spatial data with census data
m <- merge(m, c, by='zcta')
m$Positive[ is.na(m$Positive)] = 0
m$Total[ is.na(m$Total)] = 0
##Positive per capita
m$positiveperthou <- (m$Positive / m$population)*1000
m$positiveperthou[ is.na(m$positiveperthou)] = 0
m$positiveperthou <- format(round(m$positiveperthou, 2), nsmall = 2)
m$positiveperthou <- as.numeric(m$positiveperthou)
##Total per capita
m$totalperthou <- (m$Total / m$population)*1000
m$totalperthou[is.na(m$totalperthou)] = 0
m$totalperthou <- format(round(m$totalperthou, 2), nsmall = 2)
m$totalperthou <- as.numeric(m$totalperthou)
##Assign CRS
m <- spTransform(m, CRS("+proj=longlat +datum=WGS84 +init=epsg:4269"))
##Write as GeoJSON
writeOGR(m, "data/covid_nyc.json", layer="merged", driver="GeoJSON", overwrite_layer=TRUE)
##Get quantiles for breaks / legend
quantile(m$totalperthou, probs = seq(0, 1, .20))
##Chart--
##Link to DOHMH Github Data
urlfile="https://raw.githubusercontent.com/nychealth/coronavirus-data/master/case-hosp-death.csv"
##Load data
d <- read_csv(url(urlfile))
##Change NA to 0
d[is.na(d)] <- 0
##Rename column header (sometimes DOHMH messes this up)
d <- rename(d, 'DATE_OF_INTEREST' = 1)
##Create new columns with cumulative sums
d[,"NEW_COVID_CASE_COUNT_CUM"] <- cumsum(d$NEW_COVID_CASE_COUNT)
d[,"HOSPITALIZED_CASE_COUNT_CUM"] <- cumsum(d$HOSPITALIZED_CASE_COUNT)
d[,"DEATH_COUNT_CUM"] <- cumsum(d$DEATH_COUNT)
##Write as json
write_json(d, "data/case-hosp-death_cumulative.json", pretty = TRUE)
##Write as csv for public use
write.csv(d, "download/case-hosp-death_cumulative.csv", row.names = FALSE)
